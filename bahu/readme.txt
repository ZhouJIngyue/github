spider 爬虫 文件夹下为从百度爬取关键词相关的url的程序.
getText 获取文档 文件夹下为从指定url获取所有文档的程序.
compress 压缩 文件夹下为将一个文档压缩为字连接表的程序.
  compress0.3 没有考虑单元，单纯的移动窗口.
  compress0.5 考虑单元的压缩.
relation 关系 通过压缩后的各文件计算各关键词之间的距离.
  relation0.8 用一个文件读一个文件，用于内存不足的情况.
  relation0.9 一次性读取所有文件，快速，内存占用大.


bahu (Baidu Hundred 百度百页)
利用百度，爬取若干关键词页面各百页，使用页面内信息计算各个关键词之间的距离.
知道距离后便可对关键词进行聚类，或进行根据若干自选关键词推荐相似的关键词等操作.

工程分三步：
爬取数据->状态压缩->计算距离.
重点在状态压缩，现已想到三种方式.
1.爬取各关键词相关页面url，两关键词拥有相同url越多，则认为距离越近.
2.爬取各关键词相关页面内所有文字，进行字数统计并归一化，两关键词所有字频的差方和为距离.
3.迷幻莫测的.
  3.1
  对每个关键词
    爬取下该关键词相关页面内所有文字.
    建立字连接表[65536][65536]，存放字与字的连接.
    考虑到计算量与存储空间，可以减少字连接表大小到1000*1000.
    建立移动队列[10].
    对爬取下的所有文字
      在移动队列队首加入该字.
      从移动队列队尾减一个字.
      对移动队列中所有字
        字连接表[该字编码%1000][队首字编码%1000]++.
  对字连接表进行归一化，两个关键词字连接表内所有数的差方和为距离.
  这一操作与2相比可以包含字与字之间的前后连接信息.
  3.2
  更进一步!
  可以想象，文本流过后，字连接表中有意义的字之间会产生大量连接.
  一个内部包含大量连接的字的集群，应当被认为是一个单元.
  大小为10的移动队列，如果存放的是10个字，那永远只有长度为10的上下文.
  存放10个单元，境界就不同，算法如下.
  对每个关键词
    爬取下该关键词相关页面内所有文字.
    建立字连接表[1000][1000]，存放字与字的连接.
    建立单元移动队列[10].
    建立文字移动队列[4].
    对爬取下的所有文字
      在文字移动队列队首加入该字.
      从文字移动队列队尾减一个字.
      索引队列=文字移动队列各字编码%1000.
      当前单元=寻找单元(索引队列).
      在单元移动队列队首加入当前单元.
      从单元移动队列队尾减一个单元.
      对单元移动队列中所有单元
        对队首单元中所有字
          字连接表[该单元中所有字][该字]++.
  对字连接表进行归一化，两个关键词字连接表内所有数的差方和为距离.

  索引队列 寻找单元(索引队列)
    旧强度=连接强度(索引队列).
    建立新强度[1000].    
    对所有索引队列外的索引
      新强度[该索引]=连接强度(在索引队列队首加入该索引).
    (新强度最大值索引，新强度最大值)=max(新强度).
    if(新强度最大值>旧强度)
      return 寻找单元(在索引队列队首加入新强度最大值索引).
    else
      return 索引队列.

  强度 连接强度(索引队列)
    建立强度.
    for(i=0;i<索引队列大小;i++)
      for(j=i;j<索引队列大小;j++)
        强度+=字连接表[索引队列[i]][索引队列[j]].
    return 强度/=索引队列大小^2/2.

人工智能的一瞥.